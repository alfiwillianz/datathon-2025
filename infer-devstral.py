from llama_cpp import Llama
import json
from tqdm import tqdm
import time
import os

# ========== âš™ï¸ Environment Setup ==========
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

# ========== ğŸš€ Load GGUF Model to GPU Only ==========
llm = Llama(
    model_path="./Model/devstralQ4_K_M.gguf",
    n_gpu_layers=-1,
    n_ctx=2048,
    n_threads=os.cpu_count(),
    use_mlock=True,
    n_batch=1024,
    use_mmap=False,
    verbose=False,
    seed=42
)

print("âœ… Model loaded fully to GPU")

# ========== ğŸ“š Load Dataset ==========
with open("./Dataset/codealpaca_gsm8k_30k.jsonl") as f:
    all_data = [json.loads(line) for line in f]

# ========== ğŸ” Resume Support ==========
partial_file = "./Dataset/devstral_inference_partial.jsonl"
already_done = 0

if os.path.exists(partial_file):
    with open(partial_file) as f:
        already_done = sum(1 for _ in f)

print(f"ğŸ” Resuming from sample {already_done}")
dataset = all_data[already_done:]

print(f"ğŸ“š Loaded {len(dataset)} samples to process")

# ========== ğŸ” Inference Loop ==========
start_time = time.time()
batch_size = 32
backup_interval = 5  # Save backup every 5 batches

with tqdm(total=len(dataset), desc="ğŸš€ Inference Progress", unit="samples") as pbar:
    for i in range(0, len(dataset), batch_size):
        batch = dataset[i:i + batch_size]
        batch_results = []

        for item in batch:
            instruction = str(item.get("instruction", ""))[:4000]
            prompt = f"### Instruction:\n{instruction}\n\n### Response:\n"

            try:
                response = llm(
                    prompt,
                    max_tokens=256,
                    stop=["###", "</s>"],
                    temperature=0.1,
                    repeat_penalty=1.1,
                    top_p=0.9,
                    top_k=40
                )
                output = response["choices"][0]["text"].strip()
                
                # Extract logits info if available
                logits_info = None
                if "choices" in response and len(response["choices"]) > 0:
                    if "logits" in response["choices"][0]:
                        logits_info = response["choices"][0]["logits"]
                    elif "logprobs" in response["choices"][0]:
                        logits_info = response["choices"][0]["logprobs"]
                
            except Exception as e:
                output = f"ERROR: {str(e)[:100]}"
                logits_info = None

            batch_results.append({
                "instruction": instruction,
                "model_output": output,
                "logits_info": logits_info
            })

        # ğŸ’¾ Save every batch with failsafe
        try:
            with open(partial_file, "a") as f:
                for r in batch_results:
                    f.write(json.dumps(r) + "\n")
        except Exception as save_error:
            # Failsafe: Try to write to a backup file
            backup_file = f"./Dataset/devstral_inference_backup_{int(time.time())}.jsonl"
            try:
                with open(backup_file, "w") as bf:
                    for r in batch_results:
                        bf.write(json.dumps(r) + "\n")
                print(f"âš ï¸ Error saving to main file: {save_error}. Backup saved to {backup_file}")
            except Exception as backup_error:
                print(f"âŒ CRITICAL: Failed to save batch results: {backup_error}")
        
        # Create periodic backup
        if (i // batch_size) % backup_interval == 0 and i > 0:
            backup_file = f"./Dataset/devstral_inference_backup_{i}.jsonl"
            try:
                with open(backup_file, "w") as f:
                    with open(partial_file, "r") as source:
                        f.write(source.read())
                print(f"ğŸ“‘ Periodic backup saved: {backup_file}")
            except Exception as e:
                print(f"âš ï¸ Failed to create periodic backup: {e}")

        pbar.update(len(batch_results))

        # â± ETA display
        elapsed = time.time() - start_time
        processed = i + len(batch_results)
        avg_time = elapsed / processed if processed > 0 else 0
        remaining = avg_time * (len(dataset) - processed)
        pbar.set_postfix({
            'avg': f"{avg_time:.2f}s/sample",
            'ETA': f"{remaining / 60:.1f} min"
        })

# ========== âœ… Completion ==========
elapsed = time.time() - start_time
print(f"âœ… Inference completed in {elapsed / 60:.2f} minutes")
print(f"ğŸ§® Average time per sample: {elapsed / len(dataset):.2f} seconds")
print(f"ğŸ’¾ Results saved to: {partial_file}")
